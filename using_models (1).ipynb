{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJMnoaQJED4I",
        "outputId": "aef904b8-829d-46b7-d88d-a72769fb760c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.11)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.5.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.15.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.10.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.1)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.12.14)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Collecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m100.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.7.1) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.11)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.5.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.15.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.10.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.27.1)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.12.14)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.17.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.10/dist-packages (0.17.1)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.10/dist-packages (from textblob) (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (4.67.1)\n",
            "Collecting textstat\n",
            "  Downloading textstat-0.7.4-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting pyphen (from textstat)\n",
            "  Downloading pyphen-0.17.0-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from textstat) (75.1.0)\n",
            "Downloading textstat-0.7.4-py3-none-any.whl (105 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.1/105.1 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyphen-0.17.0-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyphen, textstat\n",
            "Successfully installed pyphen-0.17.0 textstat-0.7.4\n"
          ]
        }
      ],
      "source": [
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "!pip install nltk\n",
        "!pip install textblob\n",
        "!pip install textstat\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import spacy\n",
        "import nltk\n",
        "from textstat import textstat\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from tqdm.notebook import tqdm\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import re\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from spacy.tokens import Doc\n",
        "from textblob import TextBlob\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Load SpaCy model with GPU if available\n",
        "spacy.prefer_gpu()\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "nlp.add_pipe('sentencizer')  # Add sentencizer for sentence boundary detection"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0dQD26y-Eb33",
        "outputId": "9721a6b2-35f9-4e34-83e1-d339f76ceb27"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<spacy.pipeline.sentencizer.Sentencizer at 0x7934dc87d1c0>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, cohen_kappa_score\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, SimpleRNN\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "import joblib\n",
        "\n",
        "# Load the dataset\n",
        "dataset = pd.read_csv('Updated_Processed_Data.csv')\n",
        "\n",
        "# Features and target\n",
        "X = dataset.drop(columns=['final_score', 'essay', 'essay_id', 'clean_essay'])  # Drop non-predictive or target columns\n",
        "y = dataset['final_score']\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Save the scaler\n",
        "joblib.dump(scaler, 'scaler.pkl')\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate_model(y_true, y_pred, model_name):\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    qwk = cohen_kappa_score(y_true, np.round(y_pred), weights='quadratic')\n",
        "    print(f\"{model_name} Evaluation:\")\n",
        "    print(f\"MSE: {mse}, MAE: {mae}, QWK: {qwk}\\n\")\n",
        "    return mse, mae, qwk\n",
        "\n",
        "# Linear Regression\n",
        "lr = LinearRegression()\n",
        "lr.fit(X_train_scaled, y_train)\n",
        "lr_pred = lr.predict(X_test_scaled)\n",
        "evaluate_model(y_test, lr_pred, \"Linear Regression\")\n",
        "\n",
        "# Save Linear Regression model\n",
        "joblib.dump(lr, 'linear_regression_model.pkl')\n",
        "\n",
        "# ANN Model\n",
        "ann = Sequential([\n",
        "    Dense(64, activation='relu', input_dim=X_train_scaled.shape[1]),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(1, activation='linear')\n",
        "])\n",
        "ann.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "ann_checkpoint = ModelCheckpoint('ann_model.keras', save_best_only=True)\n",
        "ann.fit(X_train_scaled, y_train, validation_split=0.2, epochs=50, batch_size=32, callbacks=[ann_checkpoint], verbose=1)\n",
        "ann_pred = ann.predict(X_test_scaled).flatten()\n",
        "evaluate_model(y_test, ann_pred, \"ANN\")\n",
        "\n",
        "# LSTM Model\n",
        "X_train_lstm = X_train_scaled.reshape(X_train_scaled.shape[0], 1, X_train_scaled.shape[1])\n",
        "X_test_lstm = X_test_scaled.reshape(X_test_scaled.shape[0], 1, X_test_scaled.shape[1])\n",
        "\n",
        "lstm = Sequential([\n",
        "    LSTM(64, activation='tanh', input_shape=(1, X_train_scaled.shape[1]), return_sequences=False),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(1, activation='linear')\n",
        "])\n",
        "lstm.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "lstm_checkpoint = ModelCheckpoint('lstm_model.keras', save_best_only=True)\n",
        "lstm.fit(X_train_lstm, y_train, validation_split=0.2, epochs=50, batch_size=32, callbacks=[lstm_checkpoint], verbose=1)\n",
        "lstm_pred = lstm.predict(X_test_lstm).flatten()\n",
        "evaluate_model(y_test, lstm_pred, \"LSTM\")\n",
        "\n",
        "# RNN Model\n",
        "rnn = Sequential([\n",
        "    SimpleRNN(64, activation='tanh', input_shape=(1, X_train_scaled.shape[1]), return_sequences=False),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(1, activation='linear')\n",
        "])\n",
        "rnn.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "rnn_checkpoint = ModelCheckpoint('rnn_model.keras', save_best_only=True)\n",
        "rnn.fit(X_train_lstm, y_train, validation_split=0.2, epochs=50, batch_size=32, callbacks=[rnn_checkpoint], verbose=1)\n",
        "rnn_pred = rnn.predict(X_test_lstm).flatten()\n",
        "evaluate_model(y_test, rnn_pred, \"RNN\")\n",
        "\n",
        "# Function to predict essay scores using trained models\n",
        "def predict_essay_score(features):\n",
        "    # Load scaler and models\n",
        "    scaler = joblib.load('scaler.pkl')\n",
        "    lr_model = joblib.load('linear_regression_model.pkl')\n",
        "    ann_model = Sequential([\n",
        "        Dense(64, activation='relu', input_dim=len(features)),\n",
        "        Dense(32, activation='relu'),\n",
        "        Dense(1, activation='linear')\n",
        "    ])\n",
        "    ann_model.load_weights('ann_model.keras')\n",
        "    lstm_model = Sequential([\n",
        "        LSTM(64, activation='tanh', input_shape=(1, len(features)), return_sequences=False),\n",
        "        Dense(32, activation='relu'),\n",
        "        Dense(1, activation='linear')\n",
        "    ])\n",
        "    lstm_model.load_weights('lstm_model.keras')\n",
        "    rnn_model = Sequential([\n",
        "        SimpleRNN(64, activation='tanh', input_shape=(1, len(features)), return_sequences=False),\n",
        "        Dense(32, activation='relu'),\n",
        "        Dense(1, activation='linear')\n",
        "    ])\n",
        "    rnn_model.load_weights('rnn_model.keras')\n",
        "\n",
        "    # Scale features\n",
        "    features_scaled = scaler.transform([features])\n",
        "    features_scaled_lstm = features_scaled.reshape(1, 1, len(features))\n",
        "\n",
        "    # Predict using each model\n",
        "    predictions = {\n",
        "        'Linear Regression': lr_model.predict(features_scaled)[0],\n",
        "        'ANN': ann_model.predict(features_scaled)[0][0],\n",
        "        'LSTM': lstm_model.predict(features_scaled_lstm)[0][0],\n",
        "        'RNN': rnn_model.predict(features_scaled_lstm)[0][0]\n",
        "    }\n",
        "    return predictions\n",
        "\n",
        "# Function to score example essays\n",
        "def score_example_essays(example_features):\n",
        "    print(\"Scoring example essays with all trained models:\\n\")\n",
        "    for idx, features in enumerate(example_features):\n",
        "        print(f\"Example Essay {idx + 1}:\")\n",
        "        predictions = predict_essay_score(features)\n",
        "        for model_name, score in predictions.items():\n",
        "            print(f\"{model_name}: {score:.2f}\")\n",
        "        print(\"\\n\")\n",
        "\n",
        "print(\"All models trained and saved.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3pzGdU4NE5Ei",
        "outputId": "6f6fcbf1-e763-426a-983d-041cec8a7355"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear Regression Evaluation:\n",
            "MSE: 4.2607014105933905, MAE: 1.6412571645379062, QWK: 0.45173544773977703\n",
            "\n",
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 17.9077 - mae: 3.3985 - val_loss: 9.5457 - val_mae: 1.9171\n",
            "Epoch 2/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 5.6167 - mae: 1.7950 - val_loss: 5.4194 - val_mae: 1.6731\n",
            "Epoch 3/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4.1285 - mae: 1.5687 - val_loss: 3.6695 - val_mae: 1.4659\n",
            "Epoch 4/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.3552 - mae: 1.4134 - val_loss: 3.3766 - val_mae: 1.3585\n",
            "Epoch 5/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.9720 - mae: 1.3245 - val_loss: 3.4043 - val_mae: 1.3223\n",
            "Epoch 6/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.7518 - mae: 1.2650 - val_loss: 3.6917 - val_mae: 1.2839\n",
            "Epoch 7/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.6814 - mae: 1.2413 - val_loss: 3.3097 - val_mae: 1.2823\n",
            "Epoch 8/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.5658 - mae: 1.2192 - val_loss: 4.0704 - val_mae: 1.2596\n",
            "Epoch 9/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.7983 - mae: 1.2138 - val_loss: 3.5617 - val_mae: 1.2567\n",
            "Epoch 10/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.5355 - mae: 1.2040 - val_loss: 3.6836 - val_mae: 1.2328\n",
            "Epoch 11/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.4084 - mae: 1.1763 - val_loss: 3.4648 - val_mae: 1.2247\n",
            "Epoch 12/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.3488 - mae: 1.1549 - val_loss: 4.2704 - val_mae: 1.2323\n",
            "Epoch 13/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.4985 - mae: 1.1970 - val_loss: 3.5326 - val_mae: 1.2408\n",
            "Epoch 14/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.3327 - mae: 1.1504 - val_loss: 3.6440 - val_mae: 1.2013\n",
            "Epoch 15/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.3648 - mae: 1.1563 - val_loss: 3.7027 - val_mae: 1.2103\n",
            "Epoch 16/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2.3366 - mae: 1.1539 - val_loss: 3.7287 - val_mae: 1.2193\n",
            "Epoch 17/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2.4166 - mae: 1.1508 - val_loss: 3.2683 - val_mae: 1.1920\n",
            "Epoch 18/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2.2554 - mae: 1.1361 - val_loss: 3.6380 - val_mae: 1.1984\n",
            "Epoch 19/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2.3290 - mae: 1.1487 - val_loss: 2.9282 - val_mae: 1.1923\n",
            "Epoch 20/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.2924 - mae: 1.1317 - val_loss: 3.0872 - val_mae: 1.1868\n",
            "Epoch 21/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.3470 - mae: 1.1388 - val_loss: 3.1051 - val_mae: 1.2362\n",
            "Epoch 22/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.1904 - mae: 1.1246 - val_loss: 3.1633 - val_mae: 1.1841\n",
            "Epoch 23/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.3118 - mae: 1.1475 - val_loss: 3.1735 - val_mae: 1.1942\n",
            "Epoch 24/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.1728 - mae: 1.1128 - val_loss: 3.2879 - val_mae: 1.2027\n",
            "Epoch 25/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.1944 - mae: 1.1112 - val_loss: 2.8675 - val_mae: 1.1948\n",
            "Epoch 26/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.2366 - mae: 1.1264 - val_loss: 3.0798 - val_mae: 1.2105\n",
            "Epoch 27/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.3015 - mae: 1.1517 - val_loss: 2.8549 - val_mae: 1.1698\n",
            "Epoch 28/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.1690 - mae: 1.0952 - val_loss: 2.8825 - val_mae: 1.1698\n",
            "Epoch 29/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.0941 - mae: 1.0901 - val_loss: 2.5934 - val_mae: 1.1760\n",
            "Epoch 30/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.2215 - mae: 1.1091 - val_loss: 3.0671 - val_mae: 1.1718\n",
            "Epoch 31/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.1760 - mae: 1.1136 - val_loss: 2.7515 - val_mae: 1.1697\n",
            "Epoch 32/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.2212 - mae: 1.1168 - val_loss: 2.8329 - val_mae: 1.2032\n",
            "Epoch 33/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.1673 - mae: 1.1070 - val_loss: 2.7148 - val_mae: 1.1838\n",
            "Epoch 34/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.1600 - mae: 1.0999 - val_loss: 3.0300 - val_mae: 1.2046\n",
            "Epoch 35/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.1943 - mae: 1.1008 - val_loss: 2.4629 - val_mae: 1.1486\n",
            "Epoch 36/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2.0458 - mae: 1.0742 - val_loss: 2.7195 - val_mae: 1.1698\n",
            "Epoch 37/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2.0642 - mae: 1.0715 - val_loss: 2.5600 - val_mae: 1.1644\n",
            "Epoch 38/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.0721 - mae: 1.0804 - val_loss: 2.5402 - val_mae: 1.1515\n",
            "Epoch 39/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.0857 - mae: 1.0805 - val_loss: 2.6281 - val_mae: 1.1669\n",
            "Epoch 40/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.1373 - mae: 1.0898 - val_loss: 2.7506 - val_mae: 1.1867\n",
            "Epoch 41/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.1487 - mae: 1.0892 - val_loss: 2.5578 - val_mae: 1.1792\n",
            "Epoch 42/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.0442 - mae: 1.0651 - val_loss: 2.4890 - val_mae: 1.1671\n",
            "Epoch 43/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.1230 - mae: 1.0806 - val_loss: 2.5444 - val_mae: 1.1499\n",
            "Epoch 44/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.0866 - mae: 1.0769 - val_loss: 2.4740 - val_mae: 1.1757\n",
            "Epoch 45/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.0957 - mae: 1.0832 - val_loss: 2.6117 - val_mae: 1.2166\n",
            "Epoch 46/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.0168 - mae: 1.0640 - val_loss: 2.4645 - val_mae: 1.1495\n",
            "Epoch 47/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.0399 - mae: 1.0590 - val_loss: 2.4671 - val_mae: 1.1617\n",
            "Epoch 48/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.9956 - mae: 1.0563 - val_loss: 2.4608 - val_mae: 1.1746\n",
            "Epoch 49/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.0441 - mae: 1.0618 - val_loss: 2.5116 - val_mae: 1.1717\n",
            "Epoch 50/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.0825 - mae: 1.0701 - val_loss: 2.4667 - val_mae: 1.1582\n",
            "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "ANN Evaluation:\n",
            "MSE: 2.403026819229126, MAE: 1.1434084177017212, QWK: 0.7610234819092447\n",
            "\n",
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - loss: 22.2183 - mae: 3.9300 - val_loss: 4.4223 - val_mae: 1.6224\n",
            "Epoch 2/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 4.0428 - mae: 1.5408 - val_loss: 3.3165 - val_mae: 1.3917\n",
            "Epoch 3/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 3.1888 - mae: 1.3670 - val_loss: 2.9437 - val_mae: 1.3110\n",
            "Epoch 4/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 2.8477 - mae: 1.2921 - val_loss: 2.7791 - val_mae: 1.2665\n",
            "Epoch 5/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 2.7371 - mae: 1.2611 - val_loss: 2.6662 - val_mae: 1.2248\n",
            "Epoch 6/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 2.5176 - mae: 1.2097 - val_loss: 2.6634 - val_mae: 1.2216\n",
            "Epoch 7/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 2.4910 - mae: 1.1874 - val_loss: 2.5302 - val_mae: 1.1869\n",
            "Epoch 8/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 2.5206 - mae: 1.1857 - val_loss: 2.4995 - val_mae: 1.1754\n",
            "Epoch 9/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 2.3716 - mae: 1.1528 - val_loss: 2.5220 - val_mae: 1.1769\n",
            "Epoch 10/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 2.3297 - mae: 1.1548 - val_loss: 2.5478 - val_mae: 1.1803\n",
            "Epoch 11/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 2.3547 - mae: 1.1485 - val_loss: 2.5538 - val_mae: 1.1760\n",
            "Epoch 12/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 2.3943 - mae: 1.1581 - val_loss: 2.5049 - val_mae: 1.1623\n",
            "Epoch 13/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 2.3197 - mae: 1.1364 - val_loss: 2.4535 - val_mae: 1.1686\n",
            "Epoch 14/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 2.2903 - mae: 1.1405 - val_loss: 2.3947 - val_mae: 1.1482\n",
            "Epoch 15/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 2.2335 - mae: 1.1162 - val_loss: 2.4390 - val_mae: 1.1464\n",
            "Epoch 16/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 2.2153 - mae: 1.1145 - val_loss: 2.5019 - val_mae: 1.1568\n",
            "Epoch 17/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 2.1884 - mae: 1.1128 - val_loss: 2.3985 - val_mae: 1.1438\n",
            "Epoch 18/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 2.2629 - mae: 1.1059 - val_loss: 2.4016 - val_mae: 1.1476\n",
            "Epoch 19/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 2.2177 - mae: 1.1094 - val_loss: 2.4283 - val_mae: 1.1652\n",
            "Epoch 20/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 2.2134 - mae: 1.1107 - val_loss: 2.3933 - val_mae: 1.1376\n",
            "Epoch 21/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 2.1340 - mae: 1.0793 - val_loss: 2.3950 - val_mae: 1.1537\n",
            "Epoch 22/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 2.1801 - mae: 1.0967 - val_loss: 2.3955 - val_mae: 1.1468\n",
            "Epoch 23/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 2.2251 - mae: 1.1063 - val_loss: 2.3704 - val_mae: 1.1347\n",
            "Epoch 24/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 2.1667 - mae: 1.0975 - val_loss: 2.4108 - val_mae: 1.1453\n",
            "Epoch 25/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 2.1621 - mae: 1.0927 - val_loss: 2.3971 - val_mae: 1.1355\n",
            "Epoch 26/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 2.1061 - mae: 1.0784 - val_loss: 2.3871 - val_mae: 1.1357\n",
            "Epoch 27/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 2.1123 - mae: 1.0827 - val_loss: 2.3648 - val_mae: 1.1233\n",
            "Epoch 28/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 2.1125 - mae: 1.0756 - val_loss: 2.4047 - val_mae: 1.1423\n",
            "Epoch 29/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 2.1285 - mae: 1.0846 - val_loss: 2.4129 - val_mae: 1.1314\n",
            "Epoch 30/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 2.1216 - mae: 1.0728 - val_loss: 2.4414 - val_mae: 1.1462\n",
            "Epoch 31/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 2.0838 - mae: 1.0727 - val_loss: 2.3931 - val_mae: 1.1426\n",
            "Epoch 32/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 2.0540 - mae: 1.0737 - val_loss: 2.4194 - val_mae: 1.1456\n",
            "Epoch 33/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 2.1472 - mae: 1.0855 - val_loss: 2.4038 - val_mae: 1.1534\n",
            "Epoch 34/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 2.1349 - mae: 1.0795 - val_loss: 2.4039 - val_mae: 1.1414\n",
            "Epoch 35/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 2.1859 - mae: 1.0915 - val_loss: 2.3825 - val_mae: 1.1360\n",
            "Epoch 36/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 2.0404 - mae: 1.0599 - val_loss: 2.3918 - val_mae: 1.1490\n",
            "Epoch 37/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 2.0674 - mae: 1.0619 - val_loss: 2.4691 - val_mae: 1.1440\n",
            "Epoch 38/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 2.0505 - mae: 1.0673 - val_loss: 2.3880 - val_mae: 1.1389\n",
            "Epoch 39/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 2.0502 - mae: 1.0639 - val_loss: 2.4692 - val_mae: 1.1406\n",
            "Epoch 40/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 2.0198 - mae: 1.0518 - val_loss: 2.4382 - val_mae: 1.1478\n",
            "Epoch 41/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 2.0238 - mae: 1.0571 - val_loss: 2.4206 - val_mae: 1.1465\n",
            "Epoch 42/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 2.1006 - mae: 1.0667 - val_loss: 2.4354 - val_mae: 1.1468\n",
            "Epoch 43/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 2.0522 - mae: 1.0546 - val_loss: 2.4314 - val_mae: 1.1478\n",
            "Epoch 44/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 2.0402 - mae: 1.0500 - val_loss: 2.4406 - val_mae: 1.1523\n",
            "Epoch 45/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 2.0910 - mae: 1.0666 - val_loss: 2.4470 - val_mae: 1.1456\n",
            "Epoch 46/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 2.0547 - mae: 1.0567 - val_loss: 2.4249 - val_mae: 1.1397\n",
            "Epoch 47/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 1.9467 - mae: 1.0294 - val_loss: 2.4561 - val_mae: 1.1565\n",
            "Epoch 48/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 2.0723 - mae: 1.0534 - val_loss: 2.4504 - val_mae: 1.1486\n",
            "Epoch 49/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 2.0192 - mae: 1.0510 - val_loss: 2.4367 - val_mae: 1.1429\n",
            "Epoch 50/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 2.0216 - mae: 1.0524 - val_loss: 2.4878 - val_mae: 1.1767\n",
            "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
            "LSTM Evaluation:\n",
            "MSE: 2.3936915397644043, MAE: 1.1506487131118774, QWK: 0.747730923018729\n",
            "\n",
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 19.5457 - mae: 3.6158 - val_loss: 4.4431 - val_mae: 1.6327\n",
            "Epoch 2/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 3.8197 - mae: 1.5041 - val_loss: 3.2601 - val_mae: 1.3854\n",
            "Epoch 3/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.1817 - mae: 1.3690 - val_loss: 3.0688 - val_mae: 1.3484\n",
            "Epoch 4/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.0033 - mae: 1.3356 - val_loss: 2.8473 - val_mae: 1.2756\n",
            "Epoch 5/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.7547 - mae: 1.2685 - val_loss: 2.8032 - val_mae: 1.2642\n",
            "Epoch 6/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.6213 - mae: 1.2367 - val_loss: 2.7530 - val_mae: 1.2463\n",
            "Epoch 7/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.6171 - mae: 1.2324 - val_loss: 2.8171 - val_mae: 1.2700\n",
            "Epoch 8/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.5756 - mae: 1.2126 - val_loss: 2.6213 - val_mae: 1.2130\n",
            "Epoch 9/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.4052 - mae: 1.1681 - val_loss: 2.6057 - val_mae: 1.2049\n",
            "Epoch 10/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.4795 - mae: 1.1904 - val_loss: 2.6576 - val_mae: 1.2327\n",
            "Epoch 11/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.4449 - mae: 1.1912 - val_loss: 2.6643 - val_mae: 1.2021\n",
            "Epoch 12/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.4326 - mae: 1.1879 - val_loss: 2.5676 - val_mae: 1.1746\n",
            "Epoch 13/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.3945 - mae: 1.1671 - val_loss: 2.5201 - val_mae: 1.1765\n",
            "Epoch 14/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.2997 - mae: 1.1376 - val_loss: 2.5150 - val_mae: 1.1635\n",
            "Epoch 15/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.2917 - mae: 1.1419 - val_loss: 2.4733 - val_mae: 1.1730\n",
            "Epoch 16/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2.3419 - mae: 1.1495 - val_loss: 2.6068 - val_mae: 1.1772\n",
            "Epoch 17/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2.2878 - mae: 1.1274 - val_loss: 2.5151 - val_mae: 1.1941\n",
            "Epoch 18/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2.3428 - mae: 1.1434 - val_loss: 2.4827 - val_mae: 1.1677\n",
            "Epoch 19/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2.2561 - mae: 1.1283 - val_loss: 2.5309 - val_mae: 1.1826\n",
            "Epoch 20/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.2774 - mae: 1.1336 - val_loss: 2.5202 - val_mae: 1.1689\n",
            "Epoch 21/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.2177 - mae: 1.1223 - val_loss: 2.4650 - val_mae: 1.1558\n",
            "Epoch 22/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.2748 - mae: 1.1218 - val_loss: 2.5582 - val_mae: 1.1678\n",
            "Epoch 23/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.1803 - mae: 1.1021 - val_loss: 2.5821 - val_mae: 1.2032\n",
            "Epoch 24/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.1515 - mae: 1.0955 - val_loss: 2.5993 - val_mae: 1.1910\n",
            "Epoch 25/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.1876 - mae: 1.1103 - val_loss: 2.4899 - val_mae: 1.1713\n",
            "Epoch 26/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.2663 - mae: 1.1295 - val_loss: 2.6028 - val_mae: 1.1931\n",
            "Epoch 27/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.1663 - mae: 1.1046 - val_loss: 2.5129 - val_mae: 1.1827\n",
            "Epoch 28/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.1401 - mae: 1.0900 - val_loss: 2.4350 - val_mae: 1.1535\n",
            "Epoch 29/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.1403 - mae: 1.0945 - val_loss: 2.4833 - val_mae: 1.1691\n",
            "Epoch 30/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.1388 - mae: 1.0911 - val_loss: 2.5481 - val_mae: 1.2019\n",
            "Epoch 31/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.1673 - mae: 1.1017 - val_loss: 2.4625 - val_mae: 1.1614\n",
            "Epoch 32/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.1743 - mae: 1.1012 - val_loss: 2.4644 - val_mae: 1.1544\n",
            "Epoch 33/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.1231 - mae: 1.0863 - val_loss: 2.4950 - val_mae: 1.1948\n",
            "Epoch 34/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2.1778 - mae: 1.0967 - val_loss: 2.5178 - val_mae: 1.1717\n",
            "Epoch 35/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2.1413 - mae: 1.0845 - val_loss: 2.4375 - val_mae: 1.1530\n",
            "Epoch 36/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2.1736 - mae: 1.0942 - val_loss: 2.5997 - val_mae: 1.1749\n",
            "Epoch 37/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.1429 - mae: 1.0894 - val_loss: 2.5000 - val_mae: 1.1652\n",
            "Epoch 38/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.1155 - mae: 1.0859 - val_loss: 2.4354 - val_mae: 1.1445\n",
            "Epoch 39/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.0895 - mae: 1.0829 - val_loss: 2.4640 - val_mae: 1.1611\n",
            "Epoch 40/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.0986 - mae: 1.0799 - val_loss: 2.4654 - val_mae: 1.1588\n",
            "Epoch 41/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.1372 - mae: 1.0853 - val_loss: 2.5551 - val_mae: 1.1874\n",
            "Epoch 42/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.1151 - mae: 1.0796 - val_loss: 2.4548 - val_mae: 1.1645\n",
            "Epoch 43/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.0742 - mae: 1.0738 - val_loss: 2.4321 - val_mae: 1.1574\n",
            "Epoch 44/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.1673 - mae: 1.0942 - val_loss: 2.5101 - val_mae: 1.1762\n",
            "Epoch 45/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.0955 - mae: 1.0639 - val_loss: 2.4191 - val_mae: 1.1508\n",
            "Epoch 46/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.2142 - mae: 1.1043 - val_loss: 2.4520 - val_mae: 1.1589\n",
            "Epoch 47/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.0609 - mae: 1.0665 - val_loss: 2.5018 - val_mae: 1.1680\n",
            "Epoch 48/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.1890 - mae: 1.0941 - val_loss: 2.4583 - val_mae: 1.1536\n",
            "Epoch 49/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.0768 - mae: 1.0745 - val_loss: 2.4476 - val_mae: 1.1540\n",
            "Epoch 50/50\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.0873 - mae: 1.0724 - val_loss: 2.4949 - val_mae: 1.1734\n",
            "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n",
            "RNN Evaluation:\n",
            "MSE: 2.390796184539795, MAE: 1.144049882888794, QWK: 0.7534861878380867\n",
            "\n",
            "All models trained and saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import spacy\n",
        "import nltk\n",
        "from textstat import textstat\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from textblob import TextBlob\n",
        "import re\n",
        "import numpy as np\n",
        "\n",
        "# Load Spacy model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Define feature extraction functions\n",
        "def calculate_readability(text):\n",
        "    return textstat.flesch_reading_ease(text)\n",
        "\n",
        "def calculate_punctuation_score(text):\n",
        "    punctuation_count = sum([1 for char in text if char in \"!?.;\"])\n",
        "    return punctuation_count / len(text.split()) if len(text.split()) > 0 else 0\n",
        "\n",
        "def calculate_vocabulary_richness(text):\n",
        "    words = word_tokenize(text)\n",
        "    unique_words = set(words)\n",
        "    return len(unique_words) / len(words) if words else 0\n",
        "\n",
        "def calculate_complex_sentence_ratio(text):\n",
        "    doc = nlp(text)\n",
        "    complex_sentences = sum(1 for sent in doc.sents if sum(1 for token in sent if token.dep_ != 'punct') > 10)\n",
        "    return complex_sentences / len(list(doc.sents)) if len(list(doc.sents)) > 0 else 0\n",
        "\n",
        "def calculate_clause_density(text):\n",
        "    doc = nlp(text)\n",
        "    clauses = sum(len(list(token.subtree)) for token in doc if token.dep_ in ('csubj', 'advcl', 'acl', 'relcl'))\n",
        "    return clauses / len(list(doc.sents)) if len(list(doc.sents)) > 0 else 0\n",
        "\n",
        "def calculate_semantic_coherence(text):\n",
        "    sentences = sent_tokenize(text)\n",
        "    if len(sentences) < 2:\n",
        "        return 0\n",
        "    embeddings = [nlp(sent).vector for sent in sentences]\n",
        "    cosine_similarities = [\n",
        "        (embeddings[i] @ embeddings[i+1].T) / (np.linalg.norm(embeddings[i]) * np.linalg.norm(embeddings[i+1]))\n",
        "        for i in range(len(embeddings) - 1)\n",
        "    ]\n",
        "    return sum(cosine_similarities) / len(cosine_similarities)\n",
        "\n",
        "def calculate_sentiment_subjectivity(text):\n",
        "    blob = TextBlob(text)\n",
        "    return blob.sentiment.subjectivity\n",
        "\n",
        "def calculate_transitional_phrase_use(text):\n",
        "    transitional_phrases = [\"however\", \"therefore\", \"moreover\", \"furthermore\", \"nevertheless\"]\n",
        "    words = word_tokenize(text.lower())\n",
        "    return sum(1 for word in words if word in transitional_phrases) / len(words) if len(words) > 0 else 0\n",
        "\n",
        "def calculate_figurative_language_use(text):\n",
        "    return len(re.findall(r\"like|as if|seems|metaphorically\", text.lower())) / len(text.split()) if len(text.split()) > 0 else 0\n",
        "\n",
        "def calculate_question_usage(text):\n",
        "    return text.count('?') / len(sent_tokenize(text)) if text else 0\n",
        "\n",
        "# Example essays\n",
        "bad_essay = \"Computers are good. They help people do things. I like computers. They are fun. Everyone uses them. They are nice.\"\n",
        "great_essay = \"In today's world, computers have revolutionized how we work and communicate. They enable efficient problem-solving and enhance productivity across diverse industries. With access to vast resources, individuals can learn and innovate at unprecedented levels. Despite some challenges, the benefits of technology far outweigh the drawbacks, and computers have undoubtedly become indispensable tools for progress.\"\n",
        "\n",
        "def extract_features(essay):\n",
        "    return [\n",
        "        0,  # Placeholder for 'Unnamed: 0', irrelevant in testing but retained for structure\n",
        "        1,  # Placeholder for 'essay_set', needs contextual value\n",
        "        len(essay),  # char_count\n",
        "        len(word_tokenize(essay)),  # word_count\n",
        "        len(sent_tokenize(essay)),  # sent_count\n",
        "        np.mean([len(word) for word in word_tokenize(essay)]) if word_tokenize(essay) else 0,  # avg_word_len\n",
        "        len(re.findall(r'\\b[a-z]+\\b', essay)),  # spell_err_count (placeholder)\n",
        "        sum(1 for token in nlp(essay) if token.pos_ == \"NOUN\"),  # noun_count\n",
        "        sum(1 for token in nlp(essay) if token.pos_ == \"ADJ\"),  # adj_count\n",
        "        sum(1 for token in nlp(essay) if token.pos_ == \"VERB\"),  # verb_count\n",
        "        sum(1 for token in nlp(essay) if token.pos_ == \"ADV\"),  # adv_count\n",
        "        calculate_readability(essay),\n",
        "        calculate_punctuation_score(essay),\n",
        "        calculate_vocabulary_richness(essay),\n",
        "        calculate_complex_sentence_ratio(essay),\n",
        "        calculate_clause_density(essay),\n",
        "        calculate_semantic_coherence(essay),\n",
        "        calculate_sentiment_subjectivity(essay),\n",
        "        calculate_transitional_phrase_use(essay),\n",
        "        calculate_figurative_language_use(essay),\n",
        "        calculate_question_usage(essay)\n",
        "    ]\n",
        "\n",
        "# Extract features for the essays\n",
        "bad_essay_features = extract_features(bad_essay)\n",
        "great_essay_features = extract_features(great_essay)\n",
        "\n",
        "# Output the extracted features\n",
        "print(\"Bad Essay Features:\", bad_essay_features)\n",
        "print(\"Great Essay Features:\", great_essay_features)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YIDIdheGZ-eJ",
        "outputId": "2be0f635-bd89-48c8-fd59-b7ad32acd6f7"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bad Essay Features: [0, 1, 114, 26, 6, 3.6538461538461537, 14, 4, 3, 4, 0, 85.05, 0.3, 0.6538461538461539, 0.0, 0.0, 0.614905059337616, 0.6, 0.0, 0.05, 0.0]\n",
            "Great Essay Features: [0, 1, 409, 63, 4, 5.650793650793651, 52, 18, 5, 9, 2, 23.93, 0.07407407407407407, 0.8095238095238095, 1.0, 0.0, 0.553426076968511, 0.95, 0.0, 0.0, 0.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, SimpleRNN, Input\n",
        "\n",
        "# Load saved models and scaler\n",
        "scaler = joblib.load('scaler.pkl')\n",
        "lr_model = joblib.load('linear_regression_model.pkl')\n",
        "\n",
        "ann_model = Sequential([\n",
        "    Input(shape=(len(bad_essay_features),)),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(1, activation='linear')\n",
        "])\n",
        "ann_model.load_weights('ann_model.keras')\n",
        "\n",
        "lstm_model = Sequential([\n",
        "    Input(shape=(1, len(bad_essay_features))),\n",
        "    LSTM(64, activation='tanh', return_sequences=False),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(1, activation='linear')\n",
        "])\n",
        "lstm_model.load_weights('lstm_model.keras')\n",
        "\n",
        "rnn_model = Sequential([\n",
        "    Input(shape=(1, len(bad_essay_features))),\n",
        "    SimpleRNN(64, activation='tanh', return_sequences=False),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(1, activation='linear')\n",
        "])\n",
        "rnn_model.load_weights('rnn_model.keras')\n",
        "\n",
        "# Scale features\n",
        "bad_essay_scaled = scaler.transform([bad_essay_features])\n",
        "great_essay_scaled = scaler.transform([great_essay_features])\n",
        "\n",
        "bad_essay_scaled_lstm = bad_essay_scaled.reshape(1, 1, len(bad_essay_features))\n",
        "great_essay_scaled_lstm = great_essay_scaled.reshape(1, 1, len(great_essay_features))\n",
        "\n",
        "# Predict scores for bad essay\n",
        "bad_essay_predictions = {\n",
        "    'Linear Regression': lr_model.predict(bad_essay_scaled)[0],\n",
        "    'ANN': ann_model.predict(bad_essay_scaled)[0][0],\n",
        "    'LSTM': lstm_model.predict(bad_essay_scaled_lstm)[0][0],\n",
        "    'RNN': rnn_model.predict(bad_essay_scaled_lstm)[0][0]\n",
        "}\n",
        "\n",
        "# Predict scores for great essay\n",
        "great_essay_predictions = {\n",
        "    'Linear Regression': lr_model.predict(great_essay_scaled)[0],\n",
        "    'ANN': ann_model.predict(great_essay_scaled)[0][0],\n",
        "    'LSTM': lstm_model.predict(great_essay_scaled_lstm)[0][0],\n",
        "    'RNN': rnn_model.predict(great_essay_scaled_lstm)[0][0]\n",
        "}\n",
        "\n",
        "# Clip predictions to range [0, 10]\n",
        "def clip_scores(predictions):\n",
        "    return {model: max(0, min(10, score)) for model, score in predictions.items()}\n",
        "\n",
        "# Extract features for the essays\n",
        "bad_essay_features = extract_features(bad_essay)\n",
        "great_essay_features = extract_features(great_essay)\n",
        "\n",
        "# Display features\n",
        "print(\"Bad Essay Features:\", bad_essay_features)\n",
        "print(\"Great Essay Features:\", great_essay_features)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Dfz7lmXcswo",
        "outputId": "d9afd8a3-bdfa-4aef-bdf3-f900bf61eccd"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 175ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 85 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7934e5a2f910> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 285ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "Bad Essay Features: [0, 1, 114, 26, 6, 3.6538461538461537, 14, 4, 3, 4, 0, 85.05, 0.3, 0.6538461538461539, 0.0, 0.0, 0.614905059337616, 0.6, 0.0, 0.05, 0.0]\n",
            "Great Essay Features: [0, 1, 409, 63, 4, 5.650793650793651, 52, 18, 5, 9, 2, 23.93, 0.07407407407407407, 0.8095238095238095, 1.0, 0.0, 0.553426076968511, 0.95, 0.0, 0.0, 0.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load saved models and scaler\n",
        "scaler = joblib.load('scaler.pkl')\n",
        "lr_model = joblib.load('linear_regression_model.pkl')\n",
        "\n",
        "ann_model = Sequential([\n",
        "    Input(shape=(len(bad_essay_features),)),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(1, activation='linear')\n",
        "])\n",
        "ann_model.load_weights('ann_model.keras')\n",
        "\n",
        "lstm_model = Sequential([\n",
        "    Input(shape=(1, len(bad_essay_features))),\n",
        "    LSTM(64, activation='tanh', return_sequences=False),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(1, activation='linear')\n",
        "])\n",
        "lstm_model.load_weights('lstm_model.keras')\n",
        "\n",
        "rnn_model = Sequential([\n",
        "    Input(shape=(1, len(bad_essay_features))),\n",
        "    SimpleRNN(64, activation='tanh', return_sequences=False),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(1, activation='linear')\n",
        "])\n",
        "rnn_model.load_weights('rnn_model.keras')\n",
        "\n",
        "# Scale features\n",
        "bad_essay_scaled = scaler.transform([bad_essay_features])\n",
        "great_essay_scaled = scaler.transform([great_essay_features])\n",
        "\n",
        "bad_essay_scaled_lstm = bad_essay_scaled.reshape(1, 1, len(bad_essay_features))\n",
        "great_essay_scaled_lstm = great_essay_scaled.reshape(1, 1, len(great_essay_features))\n",
        "\n",
        "# Predict scores for bad essay\n",
        "bad_essay_predictions = {\n",
        "    'Linear Regression': lr_model.predict(bad_essay_scaled)[0],\n",
        "    'ANN': ann_model.predict(bad_essay_scaled)[0][0],\n",
        "    'LSTM': lstm_model.predict(bad_essay_scaled_lstm)[0][0],\n",
        "    'RNN': rnn_model.predict(bad_essay_scaled_lstm)[0][0]\n",
        "}\n",
        "\n",
        "# Predict scores for great essay\n",
        "great_essay_predictions = {\n",
        "    'Linear Regression': lr_model.predict(great_essay_scaled)[0],\n",
        "    'ANN': ann_model.predict(great_essay_scaled)[0][0],\n",
        "    'LSTM': lstm_model.predict(great_essay_scaled_lstm)[0][0],\n",
        "    'RNN': rnn_model.predict(great_essay_scaled_lstm)[0][0]\n",
        "}\n",
        "\n",
        "# Clip predictions to range [0, 10]\n",
        "def clip_scores(predictions):\n",
        "    return {model: max(0, min(10, score)) for model, score in predictions.items()}\n",
        "\n",
        "bad_essay_predictions = clip_scores(bad_essay_predictions)\n",
        "great_essay_predictions = clip_scores(great_essay_predictions)\n",
        "\n",
        "# Output predictions\n",
        "print(\"Bad Essay Predictions:\", bad_essay_predictions)\n",
        "print(\"Great Essay Predictions:\", great_essay_predictions)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ui44CvOhgEMg",
        "outputId": "0d1924b4-72dc-4193-eff6-7f62dd186eb8"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "WARNING:tensorflow:6 out of the last 89 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7934c41dab90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 217ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "Bad Essay Predictions: {'Linear Regression': 2.5031762898919223, 'ANN': 0.14068669, 'LSTM': 1.8740671, 'RNN': 0.85748106}\n",
            "Great Essay Predictions: {'Linear Regression': 7.784317484317058, 'ANN': 4.6780076, 'LSTM': 1.8288155, 'RNN': 2.4256368}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "great_essay = \"In the 21st century, technology has revolutionized nearly every aspect of our lives, including the field of education. It has transformed traditional teaching methods, expanded access to knowledge, and provided new opportunities for students and educators alike. While some skeptics argue that reliance on technology diminishes critical thinking and interpersonal skills, the benefits of incorporating technology in education far outweigh the drawbacks. Technology serves as a bridge to equal educational opportunities, fosters creativity, and equips students with skills essential for the future. One of the most significant advantages of technology in education is the democratization of learning. In the past, access to quality education was often limited by geographic location, financial constraints, or a lack of resources. Today, online platforms and digital tools have made it possible for students from all corners of the globe to access top-notch educational content. Websites like Khan Academy, Coursera, and edX offer free or affordable courses taught by leading educators and institutions. Students in remote or underprivileged areas can now learn the same material as their peers in more affluent regions, leveling the playing field and fostering a sense of global community. Furthermore, technology empowers educators to create more engaging and personalized learning experiences. Traditional classrooms often rely on a one-size-fits-all approach, which may not cater to the diverse needs of students. With the advent of learning management systems and data analytics, teachers can identify individual students’ strengths and weaknesses and tailor their teaching strategies accordingly. For instance, adaptive learning software adjusts the difficulty of lessons based on a student’s performance, ensuring that they remain challenged without becoming overwhelmed. This level of personalization not only improves academic outcomes but also boosts students’ confidence and motivation. Another remarkable contribution of technology to education is its ability to enhance creativity and critical thinking. Digital tools such as graphic design software, coding platforms, and virtual reality applications allow students to express themselves in innovative ways. For example, instead of writing a traditional book report, students can create multimedia presentations, interactive websites, or even animated films to demonstrate their understanding of a topic. These creative projects encourage students to think critically, collaborate with their peers, and develop problem-solving skills—abilities that are highly valued in today’s workforce. Moreover, technology plays a crucial role in preparing students for the future. The rapid pace of technological advancement means that many of the jobs today’s students will occupy do not yet exist. Familiarity with digital tools and an understanding of emerging technologies are essential for success in a constantly evolving job market. Schools that integrate technology into their curricula equip students with the skills they need to navigate and thrive in this environment. Coding classes, for instance, teach logical reasoning and computational thinking, while digital literacy programs emphasize the responsible and effective use of technology. Despite these advantages, it is important to acknowledge the challenges associated with technology in education. Critics often point to the potential for distraction, as students may be tempted to use devices for non-educational purposes. Additionally, overreliance on technology can lead to reduced interpersonal interactions and a diminished capacity for critical thinking. However, these issues can be mitigated through proper guidance and the implementation of balanced, well-thought-out policies. Teachers and administrators play a vital role in ensuring that technology is used as a tool to enhance learning rather than a substitute for meaningful engagement. In conclusion, technology has undeniably reshaped the landscape of education, offering unprecedented opportunities for students and educators alike. By breaking down barriers to access, personalizing learning, fostering creativity, and preparing students for the future, technology has become an indispensable component of modern education. While it is essential to address its challenges, the potential benefits of integrating technology into education are too significant to ignore. As we move forward, it is imperative that we continue to explore innovative ways to harness the power of technology, ensuring that it serves as a catalyst for growth and development in education\"\n",
        "\n",
        "\n",
        "# Extract features for the essays\n",
        "bad_essay_features = extract_features(bad_essay)\n",
        "great_essay_features = extract_features(great_essay)\n",
        "\n",
        "# Display features\n",
        "print(\"Bad Essay Features:\", bad_essay_features)\n",
        "print(\"Great Essay Features:\", great_essay_features)\n",
        "\n",
        "\n",
        "# Load saved models and scaler\n",
        "scaler = joblib.load('scaler.pkl')\n",
        "lr_model = joblib.load('linear_regression_model.pkl')\n",
        "\n",
        "ann_model = Sequential([\n",
        "    Input(shape=(len(bad_essay_features),)),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(1, activation='linear')\n",
        "])\n",
        "ann_model.load_weights('ann_model.keras')\n",
        "\n",
        "lstm_model = Sequential([\n",
        "    Input(shape=(1, len(bad_essay_features))),\n",
        "    LSTM(64, activation='tanh', return_sequences=False),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(1, activation='linear')\n",
        "])\n",
        "lstm_model.load_weights('lstm_model.keras')\n",
        "\n",
        "rnn_model = Sequential([\n",
        "    Input(shape=(1, len(bad_essay_features))),\n",
        "    SimpleRNN(64, activation='tanh', return_sequences=False),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(1, activation='linear')\n",
        "])\n",
        "rnn_model.load_weights('rnn_model.keras')\n",
        "\n",
        "# Scale features\n",
        "bad_essay_scaled = scaler.transform([bad_essay_features])\n",
        "great_essay_scaled = scaler.transform([great_essay_features])\n",
        "\n",
        "bad_essay_scaled_lstm = bad_essay_scaled.reshape(1, 1, len(bad_essay_features))\n",
        "great_essay_scaled_lstm = great_essay_scaled.reshape(1, 1, len(great_essay_features))\n",
        "\n",
        "# Predict scores for bad essay\n",
        "bad_essay_predictions = {\n",
        "    'Linear Regression': lr_model.predict(bad_essay_scaled)[0],\n",
        "    'ANN': ann_model.predict(bad_essay_scaled)[0][0],\n",
        "    'LSTM': lstm_model.predict(bad_essay_scaled_lstm)[0][0],\n",
        "    'RNN': rnn_model.predict(bad_essay_scaled_lstm)[0][0]\n",
        "}\n",
        "\n",
        "# Predict scores for great essay\n",
        "great_essay_predictions = {\n",
        "    'Linear Regression': lr_model.predict(great_essay_scaled)[0],\n",
        "    'ANN': ann_model.predict(great_essay_scaled)[0][0],\n",
        "    'LSTM': lstm_model.predict(great_essay_scaled_lstm)[0][0],\n",
        "    'RNN': rnn_model.predict(great_essay_scaled_lstm)[0][0]\n",
        "}\n",
        "\n",
        "# Clip predictions to range [0, 10]\n",
        "def clip_scores(predictions):\n",
        "    return {model: max(0, min(10, score)) for model, score in predictions.items()}\n",
        "\n",
        "bad_essay_predictions = clip_scores(bad_essay_predictions)\n",
        "great_essay_predictions = clip_scores(great_essay_predictions)\n",
        "\n",
        "# Output predictions\n",
        "print(\"Bad Essay Predictions:\", bad_essay_predictions)\n",
        "print(\"Great Essay Predictions:\", great_essay_predictions)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xj0slwQsgVfG",
        "outputId": "277424ed-7e49-4fc0-d2a2-0ea91d4317c7"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bad Essay Features: [0, 1, 114, 26, 6, 3.6538461538461537, 14, 4, 3, 4, 0, 85.05, 0.3, 0.6538461538461539, 0.0, 0.0, 0.614905059337616, 0.6, 0.0, 0.05, 0.0]\n",
            "Great Essay Features: [0, 1, 4649, 737, 32, 5.423337856173677, 628, 213, 70, 94, 29, 16.93, 0.04747320061255743, 0.5006784260515604, 1.0, 6.5, 0.5454023528483606, 0.514847423818012, 0.004070556309362279, 0.004594180704441042, 0.0]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 228ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "Bad Essay Predictions: {'Linear Regression': 2.5031762898919223, 'ANN': 0.14068669, 'LSTM': 1.8740671, 'RNN': 0.85748106}\n",
            "Great Essay Predictions: {'Linear Regression': 10, 'ANN': 9.914194, 'LSTM': 1.3359907, 'RNN': 3.8745875}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "great_essay = \"The author concludes the story with this paragraph to show SaengÂ’s determination. Saeng has been through a lot and misses what its like at home. She feels different and out of place but she is still strong. Life goes on, and this paragraph shows that Saeng with move with it. The paragraph says, Â“in the spring, when the snows melt and the geese return and this hibiscus is budding, then I will take that test again.Â” Spring will come, as nothing can stop time. The snow will melt because the weather changes. The geese will fly home and the hibiscus will bloom in spring. Saeng takes a positive look at things and decides to join them. Sure she failed once but she is strong and willing to try again. If at first you donÂ’t succeed, try, try again. Saeng will always have her memories of home but she is willing to change to her new sorroundings. The concluding paragraph shows SaengÂ’s determination to succeed and survive in the new world.\"\n",
        "\n",
        "\n",
        "# Extract features for the essays\n",
        "bad_essay_features = extract_features(bad_essay)\n",
        "great_essay_features = extract_features(great_essay)\n",
        "\n",
        "# Display features\n",
        "print(\"Bad Essay Features:\", bad_essay_features)\n",
        "print(\"Great Essay Features:\", great_essay_features)\n",
        "\n",
        "\n",
        "# Load saved models and scaler\n",
        "scaler = joblib.load('scaler.pkl')\n",
        "lr_model = joblib.load('linear_regression_model.pkl')\n",
        "\n",
        "ann_model = Sequential([\n",
        "    Input(shape=(len(bad_essay_features),)),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(1, activation='linear')\n",
        "])\n",
        "ann_model.load_weights('ann_model.keras')\n",
        "\n",
        "lstm_model = Sequential([\n",
        "    Input(shape=(1, len(bad_essay_features))),\n",
        "    LSTM(64, activation='tanh', return_sequences=False),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(1, activation='linear')\n",
        "])\n",
        "lstm_model.load_weights('lstm_model.keras')\n",
        "\n",
        "rnn_model = Sequential([\n",
        "    Input(shape=(1, len(bad_essay_features))),\n",
        "    SimpleRNN(64, activation='tanh', return_sequences=False),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(1, activation='linear')\n",
        "])\n",
        "rnn_model.load_weights('rnn_model.keras')\n",
        "\n",
        "# Scale features\n",
        "bad_essay_scaled = scaler.transform([bad_essay_features])\n",
        "great_essay_scaled = scaler.transform([great_essay_features])\n",
        "\n",
        "bad_essay_scaled_lstm = bad_essay_scaled.reshape(1, 1, len(bad_essay_features))\n",
        "great_essay_scaled_lstm = great_essay_scaled.reshape(1, 1, len(great_essay_features))\n",
        "\n",
        "# Predict scores for bad essay\n",
        "bad_essay_predictions = {\n",
        "    'Linear Regression': lr_model.predict(bad_essay_scaled)[0],\n",
        "    'ANN': ann_model.predict(bad_essay_scaled)[0][0],\n",
        "    'LSTM': lstm_model.predict(bad_essay_scaled_lstm)[0][0],\n",
        "    'RNN': rnn_model.predict(bad_essay_scaled_lstm)[0][0]\n",
        "}\n",
        "\n",
        "# Predict scores for great essay\n",
        "great_essay_predictions = {\n",
        "    'Linear Regression': lr_model.predict(great_essay_scaled)[0],\n",
        "    'ANN': ann_model.predict(great_essay_scaled)[0][0],\n",
        "    'LSTM': lstm_model.predict(great_essay_scaled_lstm)[0][0],\n",
        "    'RNN': rnn_model.predict(great_essay_scaled_lstm)[0][0]\n",
        "}\n",
        "\n",
        "# Clip predictions to range [0, 10]\n",
        "def clip_scores(predictions):\n",
        "    return {model: max(0, min(10, score)) for model, score in predictions.items()}\n",
        "\n",
        "bad_essay_predictions = clip_scores(bad_essay_predictions)\n",
        "great_essay_predictions = clip_scores(great_essay_predictions)\n",
        "\n",
        "# Output predictions\n",
        "print(\"Bad Essay Predictions:\", bad_essay_predictions)\n",
        "print(\"Great Essay Predictions:\", great_essay_predictions)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bGaDWN3GiZRn",
        "outputId": "d908fc60-f403-4d67-cc30-571b5bf406db"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bad Essay Features: [0, 1, 114, 26, 6, 3.6538461538461537, 14, 4, 3, 4, 0, 85.05, 0.3, 0.6538461538461539, 0.0, 0.0, 0.614905059337616, 0.6, 0.0, 0.05, 0.0]\n",
            "Great Essay Features: [0, 1, 945, 198, 12, 3.919191919191919, 155, 32, 9, 29, 10, 92.02, 0.07647058823529412, 0.5454545454545454, 0.6, 2.3333333333333335, 0.4758890894326297, 0.5879564879564879, 0.0, 0.0058823529411764705, 0.0]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 232ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "Bad Essay Predictions: {'Linear Regression': 2.5031762898919223, 'ANN': 0.14068669, 'LSTM': 1.8740671, 'RNN': 0.85748106}\n",
            "Great Essay Predictions: {'Linear Regression': 9.218299661726547, 'ANN': 3.817739, 'LSTM': 0.50579363, 'RNN': 2.6469889}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "great_essay = \"Dear is a rumor going around saying that computers are a negitive affect in a childs life. I disagree. A computer can help a kid in so many ways, it can help students with homework, let students talk to friends, and help with school projects. And besides, most kids don't don't stay on the computer very long anyways. Homeworks, it's a big role in school, if you don't do it or worse don't get it can you down. That's why computers have websites that help a child learn how do do a problem or a social studies question if they don't know. If you go to the school web page then theres a link that brings you in a math games. Theres even a to a science website. This helps a child so they don't get stressed out over homework. A computer also helps a students interact with friends. A social life is very important to a grater. This is where a computer can have aim, facebook, and even myspace help a kid impact with friends. If a kid got into a fight at school can come home and to work it out and not leave it unseatled. A computer also new friends from other towns. Teens need to and make new friesnd will helps as in life. If a teen helps with a then they can ask a for help. Every student gets a take home project at social point in the time of running out to the store to buy paper and pencils. you can stay home, have money and typee the essay. Computers can help with a social studies or a science project. You can print pictures and find information. Drawing can be a hastle and messy, so it saves time and the mess. If are words you dont understandd then you can look them out. Computers are and best way to do a project. Computers, everyone uses them, they help our parents just as much as uss. They help us with homework, let us talk to friends, and even help us do our projects! Computers are one of the best technology we have\"\n",
        "\n",
        "\n",
        "# Extract features for the essays\n",
        "bad_essay_features = extract_features(bad_essay)\n",
        "great_essay_features = extract_features(great_essay)\n",
        "\n",
        "# Display features\n",
        "print(\"Bad Essay Features:\", bad_essay_features)\n",
        "print(\"Great Essay Features:\", great_essay_features)\n",
        "\n",
        "\n",
        "# Load saved models and scaler\n",
        "scaler = joblib.load('scaler.pkl')\n",
        "lr_model = joblib.load('linear_regression_model.pkl')\n",
        "\n",
        "ann_model = Sequential([\n",
        "    Input(shape=(len(bad_essay_features),)),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(1, activation='linear')\n",
        "])\n",
        "ann_model.load_weights('ann_model.keras')\n",
        "\n",
        "lstm_model = Sequential([\n",
        "    Input(shape=(1, len(bad_essay_features))),\n",
        "    LSTM(64, activation='tanh', return_sequences=False),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(1, activation='linear')\n",
        "])\n",
        "lstm_model.load_weights('lstm_model.keras')\n",
        "\n",
        "rnn_model = Sequential([\n",
        "    Input(shape=(1, len(bad_essay_features))),\n",
        "    SimpleRNN(64, activation='tanh', return_sequences=False),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(1, activation='linear')\n",
        "])\n",
        "rnn_model.load_weights('rnn_model.keras')\n",
        "\n",
        "# Scale features\n",
        "bad_essay_scaled = scaler.transform([bad_essay_features])\n",
        "great_essay_scaled = scaler.transform([great_essay_features])\n",
        "\n",
        "bad_essay_scaled_lstm = bad_essay_scaled.reshape(1, 1, len(bad_essay_features))\n",
        "great_essay_scaled_lstm = great_essay_scaled.reshape(1, 1, len(great_essay_features))\n",
        "\n",
        "# Predict scores for bad essay\n",
        "bad_essay_predictions = {\n",
        "    'Linear Regression': lr_model.predict(bad_essay_scaled)[0],\n",
        "    'ANN': ann_model.predict(bad_essay_scaled)[0][0],\n",
        "    'LSTM': lstm_model.predict(bad_essay_scaled_lstm)[0][0],\n",
        "    'RNN': rnn_model.predict(bad_essay_scaled_lstm)[0][0]\n",
        "}\n",
        "\n",
        "# Predict scores for great essay\n",
        "great_essay_predictions = {\n",
        "    'Linear Regression': lr_model.predict(great_essay_scaled)[0],\n",
        "    'ANN': ann_model.predict(great_essay_scaled)[0][0],\n",
        "    'LSTM': lstm_model.predict(great_essay_scaled_lstm)[0][0],\n",
        "    'RNN': rnn_model.predict(great_essay_scaled_lstm)[0][0]\n",
        "}\n",
        "\n",
        "# Clip predictions to range [0, 10]\n",
        "def clip_scores(predictions):\n",
        "    return {model: max(0, min(10, score)) for model, score in predictions.items()}\n",
        "\n",
        "bad_essay_predictions = clip_scores(bad_essay_predictions)\n",
        "great_essay_predictions = clip_scores(great_essay_predictions)\n",
        "\n",
        "# Output predictions\n",
        "print(\"Bad Essay Predictions:\", bad_essay_predictions)\n",
        "print(\"Great Essay Predictions:\", great_essay_predictions)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tO4Mz6rZjTHt",
        "outputId": "b4ee1b3a-520d-43c1-ba55-44098f674eb9"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bad Essay Features: [0, 1, 114, 26, 6, 3.6538461538461537, 14, 4, 3, 4, 0, 85.05, 0.3, 0.6538461538461539, 0.0, 0.0, 0.614905059337616, 0.6, 0.0, 0.05, 0.0]\n",
            "Great Essay Features: [0, 1, 1838, 404, 26, 3.6683168316831685, 340, 86, 22, 56, 19, 90.8, 0.0700280112044818, 0.42574257425742573, 0.6538461538461539, 5.923076923076923, 0.38767436414957046, 0.3217323232323232, 0.0, 0.0, 0.0]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 218ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "Bad Essay Predictions: {'Linear Regression': 2.5031762898919223, 'ANN': 0.14068669, 'LSTM': 1.8740671, 'RNN': 0.85748106}\n",
            "Great Essay Predictions: {'Linear Regression': 10, 'ANN': 5.570169, 'LSTM': 0.8563629, 'RNN': 3.8242378}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dMhsfza6jgQI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}